# Scalable Search Engine System

A production-ready, web-scale search engine architecture capable of crawling billions of pages and handling billions of queries per month.

[![Python 3.13](https://img.shields.io/badge/python-3.13-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-green.svg)](https://fastapi.tiangolo.com/)
[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)
[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](http://mypy-lang.org/)

## ğŸ“‹ Requirements

The system must handle:
- **4 billion pages crawled per month** (~1,500 pages/second)
- **~100 billion search queries per month** (~38,000 queries/second)  
- **On-demand re-crawl requests** with 1-hour SLA

## ğŸ—ï¸ Architecture Overview

### Core Components

1. **API Layer (FastAPI)**
   - RESTful API with auto-generated OpenAPI docs
   - Async/await for high concurrency
   - Horizontal scaling: 100-200 instances
   - API key authentication & rate limiting

2. **Cache Layer (Redis)**
   - Query result caching
   - 70-80% cache hit rate target
   - Distributed rate limiting
   - Sub-millisecond response times

3. **Search Engine (Elasticsearch)**
   - Inverted index for 4 billion documents
   - Distributed across 50-100 nodes
   - Full-text search with BM25 ranking
   - Sharded for horizontal scaling

4. **Database (PostgreSQL)**
   - Document metadata storage
   - Job status tracking with SLA monitoring
   - Connection pooling for high throughput

5. **Message Queue (RabbitMQ)**
   - Asynchronous job processing
   - Priority queues for re-crawls
   - Reliable message delivery

6. **Workers**
   - **Crawler**: Downloads web pages (1,500-2,000 workers)
   - **Processor**: Extracts and tokenizes text
   - **Indexer**: Builds inverted index in Elasticsearch

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose (recommended)
- Python 3.13+ (for local development)
- Git

### Option 1: Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/Haseeb717/search_engine_system
cd search-engine-system

# Start all services
docker-compose up -d

# Check service health
curl http://localhost:8000/health

# View logs
docker-compose logs -f api
```

### Option 2: Local Development

```bash
# Clone the repository
git clone https://github.com/Haseeb717/search_engine_system
cd search-engine-system

# Create virtual environment
python3.13 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment variables
cp .env.example .env

# Edit .env and set all *_HOST values to 'localhost'
# Then start services manually (Redis, Elasticsearch, PostgreSQL, RabbitMQ)

# Run the API
python -m uvicorn app.main:app --reload
```

### Verify Installation

```bash
# API should be running
curl http://localhost:8000/

# Check all services
curl http://localhost:8000/health

# Access interactive API documentation
open http://localhost:8000/docs
```

## ğŸ“¡ API Endpoints

### 1. Search

**POST** `/api/v1/search`

Search through billions of indexed web pages.

```bash
curl -X POST http://localhost:8000/api/v1/search \
  -H "Content-Type: application/json" \
  -H "X-API-Key: demo-key-12345" \
  -d '{
    "query": "python tutorial",
    "page": 1,
    "page_size": 10
  }'
```

**Response:**
```json
{
  "query": "python tutorial",
  "total_results": 1250,
  "page": 1,
  "page_size": 10,
  "total_pages": 125,
  "results": [
    {
      "url": "https://example.com/python",
      "title": "Python Tutorial",
      "snippet": "Learn Python programming...",
      "domain": "example.com",
      "crawl_date": "2024-11-29T12:00:00",
      "score": 0.95
    }
  ],
  "search_time_ms": 45.2,
  "cached": false
}
```

### 2. Re-crawl Request

**POST** `/api/v1/crawl/recrawl`

Request urgent re-crawl with 1-hour SLA.

```bash
curl -X POST http://localhost:8000/api/v1/crawl/recrawl \
  -H "Content-Type: application/json" \
  -H "X-API-Key: demo-key-12345" \
  -d '{
    "url": "https://example.com/page",
    "priority": 10
  }'
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "message": "Re-crawl job created successfully",
  "created_at": "2024-11-29T12:00:00",
  "sla_deadline": "2024-11-29T13:00:00"
}
```

### 3. Job Status

**GET** `/api/v1/jobs/{job_id}`

Check crawl/re-crawl job status.

```bash
curl http://localhost:8000/api/v1/jobs/550e8400-e29b-41d4-a716-446655440000 \
  -H "X-API-Key: demo-key-12345"
```

**Response:**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "job_type": "recrawl",
  "url": "https://example.com/page",
  "status": "completed",
  "priority": 10,
  "created_at": "2024-11-29T12:00:00",
  "started_at": "2024-11-29T12:00:30",
  "completed_at": "2024-11-29T12:15:00",
  "sla_deadline": "2024-11-29T13:00:00",
  "result": {
    "pages_crawled": 1,
    "documents_indexed": 1
  }
}
```

## ğŸ”§ Configuration

Environment variables are configured in `.env` file:

```bash
# Copy example file
cp .env.example .env
```

Key configurations:

```env
# Application
APP_ENV=development
DEBUG=True
API_HOST=0.0.0.0
API_PORT=8000

# Redis (Cache)
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_CACHE_TTL=1800

# Elasticsearch (Search)
ELASTICSEARCH_HOST=elasticsearch
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_INDEX=web_pages

# PostgreSQL (Database)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=search_engine

# RabbitMQ (Queue)
RABBITMQ_HOST=rabbitmq
RABBITMQ_PORT=5672
```

## ğŸ§ª Testing

### Run All Tests

```bash
# Install test dependencies
pip install -r requirements.txt

# Run tests
python -m pytest -v

# Run with coverage
python -m pytest --cov=app --cov-report=html
open htmlcov/index.html
```

### Run Specific Tests

```bash
# Test search endpoint
python -m pytest tests/test_api_search.py -v

# Test re-crawl endpoint
python -m pytest tests/test_api_crawl.py -v

# Test job status
python -m pytest tests/test_api_jobs.py -v

# Test models only
python -m pytest tests/test_models.py -v
```

### Test Coverage

Current coverage: **80%+**

- âœ… All API endpoints
- âœ… Request/response validation
- âœ… Error handling
- âœ… Service layer logic
- âœ… Model validation

## ğŸ¨ Code Quality

### Linting with Ruff

```bash
# Install linting tools
pip install ruff==0.5.0

# Check code
ruff check .

# Auto-fix issues
ruff check --fix .

# Format code
ruff format .
```

### Type Checking with Mypy

```bash
# Install type checking tools
pip install mypy==1.10.0 types-redis==4.6.0.20240218 types-pika==1.2.0b1

# Run type checking
mypy app/

# Check specific file
mypy app/main.py
```

### Pre-commit Checks

```bash
# Run all quality checks before committing
ruff check . && \
ruff format --check . && \
mypy app/ && \
python -m pytest
```

## ğŸ“Š Scaling Strategy

### How It Scales to Requirements

#### Search Queries (100 billion/month = 38,000 queries/sec)

**Solution:**
1. **Caching (80% hit rate)**
   - 30,400 queries/sec hit Redis cache (instant)
   - 7,600 queries/sec hit Elasticsearch

2. **Horizontal API Scaling**
   - 100 FastAPI instances Ã— 400 req/sec = 40,000 req/sec capacity

3. **Elasticsearch Cluster**
   - 50-100 nodes working in parallel
   - Data sharded across nodes

#### Crawling (4 billion pages/month = 1,500 pages/sec)

**Solution:**
1. **Parallel Workers**
   - 1,500-2,000 crawler workers
   - Each downloads ~1 page/second
   - Auto-scaling based on queue depth

2. **Queue-Based Architecture**
   - RabbitMQ handles job distribution
   - Persistent queues (no job loss)

#### 1-Hour Re-crawl SLA

**Solution:**
1. **Dedicated Worker Pool**
   - Separate high-priority queue
   - 100-200 workers reserved for re-crawls
   - Priority routing ensures immediate processing

2. **SLA Monitoring**
   - Track job creation to completion time
   - Alert if approaching deadline
   - Auto-scale workers if needed

## ğŸ­ Production Deployment

### Kubernetes Deployment

```yaml
# api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: search-api
spec:
  replicas: 10
  selector:
    matchLabels:
      app: search-api
  template:
    metadata:
      labels:
        app: search-api
    spec:
      containers:
      - name: api
        image: search-engine-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: REDIS_HOST
          value: redis-service
        - name: ELASTICSEARCH_HOST
          value: elasticsearch-service
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
```

### Horizontal Pod Autoscaler

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: search-api
  minReplicas: 10
  maxReplicas: 200
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### Scaling Workers

```bash
# Docker Compose
docker-compose up -d --scale crawler=100

# Kubernetes
kubectl scale deployment crawler --replicas=1000
```

## ğŸ“ˆ Monitoring

### Metrics

- API response times
- Cache hit rates
- Queue depths
- Worker throughput
- Error rates

### Endpoints

- `/health` - Service health check
- `/metrics` - Prometheus metrics (optional)
- `/docs` - OpenAPI documentation

### Access Dashboards

```bash
# RabbitMQ Management UI
open http://localhost:15672
# Username: guest, Password: guest

# Elasticsearch
curl http://localhost:9200/_cluster/health

# API Docs
open http://localhost:8000/docs
```

## ğŸ“ Project Structure

```
search-engine-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry point
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”‚   â”œâ”€â”€ auth.py            # Authentication middleware
â”‚   â”‚   â””â”€â”€ rate_limit.py      # Rate limiting
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚       â”œâ”€â”€ search.py      # Search endpoint
â”‚   â”‚       â”œâ”€â”€ crawl.py       # Re-crawl endpoint
â”‚   â”‚       â””â”€â”€ jobs.py        # Job status endpoint
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ requests.py        # Pydantic request models
â”‚   â”‚   â””â”€â”€ responses.py       # Pydantic response models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ cache_service.py   # Caching logic
â”‚   â”‚   â”œâ”€â”€ search_service.py  # Search business logic
â”‚   â”‚   â””â”€â”€ crawl_service.py   # Job management logic
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ redis_client.py    # Redis connection
â”‚   â”‚   â”œâ”€â”€ elasticsearch_client.py  # ES connection
â”‚   â”‚   â”œâ”€â”€ postgres_client.py # PostgreSQL with SQLAlchemy
â”‚   â”‚   â””â”€â”€ rabbitmq_client.py # RabbitMQ connection
â”‚   â””â”€â”€ workers/
â”‚       â”œâ”€â”€ crawler.py         # Crawler worker
â”‚       â”œâ”€â”€ processor.py       # Text processor
â”‚       â””â”€â”€ indexer.py         # Indexer worker
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py            # Test fixtures
â”‚   â”œâ”€â”€ test_api_search.py     # Search endpoint tests
â”‚   â”œâ”€â”€ test_api_crawl.py      # Re-crawl endpoint tests
â”‚   â”œâ”€â”€ test_api_jobs.py       # Job status tests
â”‚   â”œâ”€â”€ test_health.py         # Health check tests
â”‚   â”œâ”€â”€ test_models.py         # Model validation tests
â”‚   â””â”€â”€ test_services.py       # Service layer tests
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ system_architecture.png  # Architecture diagram
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # Detailed architecture docs
â”‚   â””â”€â”€ API_SPECIFICATION.md   # Complete API specification
â”œâ”€â”€ docker-compose.yml         # Docker services configuration
â”œâ”€â”€ Dockerfile                 # Container definition
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ pytest.ini                 # Pytest configuration
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ .gitignore                # Git ignore rules
â””â”€â”€ README.md                 # This file
```

## ğŸ”‘ Key Design Decisions

### Why FastAPI?
- Native async/await support (high concurrency)
- Automatic OpenAPI documentation
- Type safety with Pydantic
- Fast performance (comparable to Node.js/Go)

### Why Redis for Caching?
- In-memory speed (sub-millisecond latency)
- Cluster mode for horizontal scaling
- 80% cache hit rate reduces database load by 80%

### Why Elasticsearch?
- Purpose-built for full-text search
- Inverted index architecture
- Distributed by design
- Relevance scoring (BM25 algorithm)

### Why RabbitMQ?
- Reliable message delivery
- Priority queues
- Dead letter queues for failure handling
- Proven at scale

### Why PostgreSQL?
- ACID compliance for metadata
- Mature sharding support
- Reliable for job tracking

## ğŸš¦ API Features

### Authentication

```bash
# Include API key in header
curl -H "X-API-Key: demo-key-12345" http://localhost:8000/api/v1/search
```

Demo API keys:
- `demo-key-12345` - 1000 requests/minute
- `test-key-67890` - 100 requests/minute

### Rate Limiting

- Default: 1000 requests/minute per API key
- Returns `429 Too Many Requests` when exceeded
- Headers include rate limit info:
  - `X-RateLimit-Limit`
  - `X-RateLimit-Remaining`
  - `X-RateLimit-Reset`

### Pagination

- Default: 10 results per page
- Max: 100 results per page
- Use `page` and `page_size` parameters

### Error Handling

All errors follow consistent format:

```json
{
  "error": "ErrorType",
  "message": "Human-readable message",
  "detail": "Additional details"
}
```

Status codes:
- `200 OK` - Success
- `202 Accepted` - Job created
- `400 Bad Request` - Invalid input
- `401 Unauthorized` - Invalid API key
- `404 Not Found` - Resource not found
- `429 Too Many Requests` - Rate limit exceeded
- `500 Internal Server Error` - Server error
- `503 Service Unavailable` - Service down

## ğŸ¤ Contributing

```bash
# Fork the repository
# Create a feature branch
git checkout -b feature/amazing-feature

# Make changes and run quality checks
ruff check . && ruff format . && mypy app/ && python -m pytest

# Commit changes
git commit -m "Add amazing feature"

# Push to branch
git push origin feature/amazing-feature

# Open a Pull Request
```

## ğŸ“ License

This project is part of a technical assessment.

## ğŸ“§ Contact

For questions about this system, contact: careers@forager.ai

---

**Built with â¤ï¸ for scalable search at web scale**

## ğŸ¯ Assignment Deliverables

This project includes all required deliverables:

1. âœ… **System Diagram** - See `diagrams/system_architecture.png` and `docs/ARCHITECTURE.md`
2. âœ… **API Specification** - See `docs/API_SPECIFICATION.md` and `/docs` endpoint
3. âœ… **API Code** - Complete FastAPI implementation with:
   - Separation of concerns (routes, services, models, workers)
   - Scalability demonstrations (caching, async, queues)
   - Production-ready patterns (auth, rate limiting, error handling)
   - Comprehensive tests (80%+ coverage)

### Quick Demo

```bash
# Start everything
docker-compose up -d

# View API docs
open http://localhost:8000/docs

# Test search
curl -X POST http://localhost:8000/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{"query": "test", "page": 1, "page_size": 10}'

# Request re-crawl
curl -X POST http://localhost:8000/api/v1/crawl/recrawl \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "priority": 10}'
```